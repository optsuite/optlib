/-
Copyright (c) 2023 Wanyi He. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Author: Wanyi He
-/
import Mathlib.Analysis.InnerProductSpace.PiL2
import Mathlib.Topology.MetricSpace.Basic
import Mathlib.Topology.Basic
import Mathlib.Analysis.Convex.Function
import Analysis.Basic
import Mathlib.Topology.MetricSpace.PseudoMetric
import Function.Convex_Function

/-!
# Subgradient of convex functions in EuclideanSpace

The file defines subgradient for convex functions in E and proves some basic properties.

Let `f : E â†’ â„` be a convex function on `s` and `g : E`,
where `s` is a set of `E`. Suppose `hf : ConvexOn â„ s f`.
`g` is a subgradient of `f` at `x` if for any `y âˆˆ s`, we have `f y â‰¥ f x + inner g (y - x)`.
The insight comes from the first order condition of convex function.

## Main declarations

* `IsSubgradAt hf g x`: The convex function `f` has subgradient `g` at `x`.
Here `f` is given as an implicit argument
* `SubderivAt hf x`: The collection of all possible subgradients of `f` at `x`.

## Main results

* `subgrad_of_grad` : If `f` has Fderiv `f' x` at `x`, then `SubderivAt hf x = {grad (f' x)}`.
* `zero_mem_iff_isGlobalmin` : Optimality conditions for convex objective functions
-/

open Filter Topology Set InnerProductSpace


noncomputable section

variable {n : Type _} [Fintype n] [DecidableEq n]

variable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace â„ E] [CompleteSpace E]

variable {s : Set E}

variable {f : E â†’ â„} {g : E} {x : E}

variable {f' : E â†’ (E â†’L[â„] â„)}

set_option quotPrecheck false

local notation gradient "âˆ‡*" => (toDualMap â„ _) gradient

local notation "âŸª" x ", " y "âŸ«" => @inner â„ _ _ x y

/-- Subgradient of functions --/
def IsSubgradAt (_ : ConvexOn â„ s f) (g x : E) : Prop :=
  âˆ€ y âˆˆ s, f y â‰¥ f x + inner g (y - x)

/-- Subderiv of functions --/
def SubderivAt (hf : ConvexOn â„ s f) (x :  E) : Set E :=
  {g : E| IsSubgradAt hf g x}

@[simp]
theorem mem_SubderivAt (hf : ConvexOn â„ s f) : IsSubgradAt hf g x â†” g âˆˆ SubderivAt hf x := âŸ¨id, idâŸ©

/-! ### Basic properties about `Subderiv` -/

open EuclideanSpace Set

variable (hf : ConvexOn â„ s f)

/-- The subderiv of `f` at `x` is a closed set. --/
theorem Subderiv.isClosed : âˆ€ x âˆˆ s, IsClosed (SubderivAt hf x) := by
  intro x _
  by_cases e : SubderivAt hf x = âˆ…
  Â· apply Eq.subst (Eq.symm e) isClosed_empty
  rw [â† isSeqClosed_iff_isClosed]
  intro g g' hg cg y ys
  obtain cg' := Tendsto.const_add (f x) (Filter.Tendsto.inner cg tendsto_const_nhds)
  apply le_of_tendsto_of_tendsto' cg' tendsto_const_nhds (fun n => hg n y ys)

/-- The subderiv of `f` at `x` is a convex set. --/
theorem Subderiv.convex : âˆ€ x âˆˆ s, Convex â„ (SubderivAt hf x) := by
  intro x _
  by_cases e : SubderivAt hf x = âˆ…
  Â· apply Eq.subst (Eq.symm e) convex_empty
  intro gâ‚ h1 gâ‚‚ h2 a b lea leb abeq y ys
  have ineq1 : a â€¢ f y â‰¥ a â€¢ f x + a â€¢ âŸªgâ‚, y - xâŸ« := by
    rw [â† smul_add]
    apply smul_le_smul_of_nonneg (h1 y ys) lea
  have ineq2 : b â€¢ f y â‰¥ b â€¢ f x + b â€¢ inner gâ‚‚ (y - x) := by
    rw [â† smul_add]
    apply smul_le_smul_of_nonneg (h2 y ys) leb
  have eq : (a â€¢ f x + a â€¢ inner gâ‚ (y - x)) + (b â€¢ f x + b â€¢ inner gâ‚‚ (y - x))
      = f x + inner (a â€¢ gâ‚ + b â€¢ gâ‚‚) (y - x) := by
    rw [add_add_add_comm, â† Eq.symm (Convex.combo_self abeq (f x))]
    apply congrArg (HAdd.hAdd (f x))
    rw [inner_add_left, inner_smul_left, inner_smul_left]; rfl
  rw [Eq.symm (Convex.combo_self abeq (f y)), â† eq]
  apply add_le_add ineq1 ineq2


/-- Monotonicity of subderiv--/
theorem subgrad_mono {u v : E} (hf : ConvexOn â„ s f) (xs : x âˆˆ s) (ys : y âˆˆ s)
  (hu : u âˆˆ SubderivAt hf x) (hv : v âˆˆ SubderivAt hf y) :
    âŸªu - v, x - yâŸ« â‰¥ (0 : â„):= by
      specialize hu y ys; specialize hv x xs
      have ineq1 : âŸªu, x - yâŸ« â‰¥ f x - f y := by
        rw [congrArg (inner u) (Eq.symm (neg_sub y x)), inner_neg_right]; linarith
      have ineq2 := Iff.mpr le_sub_iff_add_le' hv
      rw [inner_sub_left]; linarith


/-! ### Calculation of `Subderiv` -/

open Pointwise

lemma first_order_condition_gradn {f: E â†’ â„} {gradf : E}
  {s : Set E} {x: E} (h: HasGradientAt f gradf x) (hf: ConvexOn â„ s f) (xs: xâˆˆ s) :
  âˆ€ (y : E), y âˆˆ s â†’ f x + inner gradf (y - x) â‰¤ f y:= by
  have H1: âˆ€ (y : E), y âˆˆ s â†’ f x + (gradf âˆ‡*) (y - x) â‰¤ f y:= by
    rw [HasGradientAt] at h
    apply first_order_condition; apply h;
    apply hf; apply xs
  intro y ys
  specialize H1 y ys
  exact H1

/-- Subderiv of differentiable functions --/
theorem subgrad_of_grad' (hx : x âˆˆ interior s) (hf : ConvexOn â„ s f) (h : HasGradientAt f g x) :
  SubderivAt hf x = {g} := by
  obtain h' := HasGradientAt_iff_HasFDerivAt.mp h
  rw [Set.eq_singleton_iff_nonempty_unique_mem]
  constructor
  Â· use g; intro y ys
    exact first_order_condition_gradn h hf (interior_subset hx) y ys
  intro g' hg'; by_contra neq
  apply not_le_of_lt (norm_sub_pos_iff.mpr neq)
  let v := g' - g; obtain vneq := sub_ne_zero.mpr neq
  have : Tendsto (fun (t : â„) => (f (x + t â€¢ v) - f x - âŸªg, t â€¢ vâŸ«) * â€–t â€¢ vâ€–â»Â¹) (ğ“[>] 0) (ğ“ 0) := by
    rw [Metric.tendsto_nhdsWithin_nhds]; intro Îµ Îµpos
    unfold HasFDerivAt at h'
    rw [hasFDerivAtFilter_iff_tendsto, Metric.tendsto_nhds_nhds] at h'
    obtain âŸ¨Î´, Î´pos, hÎ´âŸ© := h' Îµ Îµpos
    use (Î´ * â€–vâ€–â»Â¹)
    obtain pos := mul_pos Î´pos (inv_pos.mpr (norm_pos_iff.mpr vneq))
    constructor
    Â· exact pos
    intro t _ ht; rw [dist_eq_norm] at ht; rw [dist_eq_norm]
    have : dist (x + t â€¢ v) x < Î´ := by
      rw [dist_eq_norm, add_sub_cancel', norm_smul, â† (sub_zero t)]
      apply lt_of_lt_of_eq ((mul_lt_mul_right (norm_sub_pos_iff.mpr neq)).mpr ht)
      rw [mul_assoc, inv_mul_cancel (norm_ne_zero_iff.mpr vneq), mul_one]
    specialize hÎ´ this; rw [dist_eq_norm] at hÎ´
    have eq1 : â€–â€–x + t â€¢ v - xâ€–â»Â¹â€– = â€–t â€¢ vâ€–â»Â¹ := by
      rw [add_sub_cancel', norm_inv, norm_norm]
    have eq2 : (g âˆ‡*) (x + t â€¢ v - x) = âŸªg, t â€¢ vâŸ« := by rw [add_sub_cancel']; exact rfl
    have eq3 : â€–(f (x + t â€¢ v) - f x - âŸªg, t â€¢ vâŸ«) * â€–t â€¢ vâ€–â»Â¹ - 0â€– =
      â€–f (x + t â€¢ v) - f x - âŸªg, t â€¢ vâŸ«â€– * â€–t â€¢ vâ€–â»Â¹ := by
        rw [sub_zero, norm_mul, norm_inv, norm_norm]
    have eq : â€–â€–x + t â€¢ v - xâ€–â»Â¹ * â€–f (x + t â€¢ v) - f x - (g âˆ‡*) (x + t â€¢ v - x)â€– - 0â€– =
      â€–(f (x + t â€¢ v) - f x - âŸªg, t â€¢ vâŸ«) * â€–t â€¢ vâ€–â»Â¹ - 0â€– := by
        rw [sub_zero, norm_mul, norm_norm, mul_comm, eq1, eq2, â† eq3]
    apply Eq.trans_lt (Eq.symm eq) hÎ´
  apply ge_of_tendsto this
  rw [Filter.Eventually, Metric.mem_nhdsWithin_iff]
  rw [mem_interior_iff_mem_nhds, Metric.mem_nhds_iff] at hx
  obtain âŸ¨Îµ, Îµpos, ballmemâŸ© := hx
  obtain pos := mul_pos Îµpos (inv_pos.mpr (norm_pos_iff.mpr vneq))
  use (Îµ * â€–vâ€–â»Â¹); use pos; intro t ht
  have tball := ht.1; have tlt : t > 0 := ht.2
  have tvpos : â€–t â€¢ vâ€–â»Â¹ > 0 := by
    apply inv_pos.mpr; rw [norm_smul]
    apply smul_pos (abs_pos_of_pos tlt) (norm_sub_pos_iff.mpr neq)
  have mems : x + t â€¢ v âˆˆ s := by
    apply ballmem
    rw [mem_ball_iff_norm, sub_zero] at tball
    rw [mem_ball_iff_norm, add_sub_cancel', norm_smul]
    have : â€–tâ€– * â€–vâ€– < Îµ * â€–vâ€–â»Â¹ * â€–vâ€– := by
      apply (mul_lt_mul_right (norm_sub_pos_iff.mpr neq)).mpr tball
    rwa [mul_assoc, inv_mul_cancel (norm_ne_zero_iff.mpr vneq), mul_one] at this
  obtain ineq1 := hg' (x + t â€¢ v) mems; rw [add_sub_cancel'] at ineq1
  have eq1 : â€–vâ€– = (âŸªg', t â€¢ vâŸ« - âŸªg, t â€¢ vâŸ«) * â€–t â€¢ vâ€–â»Â¹ := by
    have eq2 : â€–vâ€– = âŸªv, vâŸ« * â€–vâ€–â»Â¹ := by
      rw [real_inner_self_eq_norm_sq]
      apply (div_eq_iff _).mp
      Â· rw [div_inv_eq_mul, pow_two]
      exact inv_ne_zero (norm_ne_zero_iff.mpr vneq)
    have eq3 : âŸªv, vâŸ« * â€–vâ€–â»Â¹ = âŸªv, t â€¢ vâŸ« * â€–t â€¢ vâ€–â»Â¹ := by
      have : âŸªv, t â€¢ vâŸ« = âŸªv, vâŸ« * t := by rw [inner_smul_right, mul_comm]
      rw [this, mul_assoc]
      have : â€–vâ€–â»Â¹ = t * â€–t â€¢ vâ€–â»Â¹ := by
        have :  t * â€–t â€¢ vâ€–â»Â¹ = t / â€–t â€¢ vâ€– := rfl
        rw [this, inv_eq_one_div]
        have : t = â€–tâ€– := by
          have : â€–tâ€– = |t| := rfl
          rw [this, abs_of_pos tlt]
        rw [this, norm_smul, norm_norm, div_mul_eq_div_div, div_self]
        rw [norm_ne_zero_iff]
        exact ne_of_gt tlt
      rw [this]
    rw [eq2, eq3, mul_eq_mul_right_iff];
    left; rw [inner_sub_left]
  rw [mem_setOf, eq1, mul_le_mul_right tvpos]
  apply sub_le_sub_right (le_sub_iff_add_le'.mpr ineq1)

/-- Alternarive version for FDeriv --/
theorem subgrad_of_grad (hx : x âˆˆ interior s) (hf : ConvexOn â„ s f) (h : HasFDerivAt f (f' x) x) :
  SubderivAt hf x = {(toDual â„ E).symm (f' x)} := by
    have hâ‚ : HasFDerivAt f ((toDual â„ E) ((LinearIsometryEquiv.symm (toDual â„ E)) (f' x))) x := by
      simp [h]
    obtain h' := HasGradientAt_iff_HasFDerivAt.mpr hâ‚
    exact subgrad_of_grad' hx hf h'

/-- Subderiv of the sum of two functions is a subset of the sum of the subderivs of the two functions --/
theorem subgrad_of_add {s t : Set E} {fâ‚ fâ‚‚ : E â†’ â„}
  (hfâ‚ : ConvexOn â„ s fâ‚) (hfâ‚‚ : ConvexOn â„ t fâ‚‚) (hadd : ConvexOn â„ (s âˆ© t) (fâ‚ + fâ‚‚)):
    âˆ€ (x : E), SubderivAt hfâ‚ x + SubderivAt hfâ‚‚ x âŠ† SubderivAt hadd x := by
      intro x y ymem; intro y' hy'
      obtain âŸ¨yâ‚, yâ‚‚, hyâ‚, hyâ‚‚, eqâŸ© := Set.mem_add.mpr ymem
      have eq' : yâ‚ + yâ‚‚ = y := eq
      have : (fâ‚ + fâ‚‚) y' â‰¥ (fâ‚ x + âŸªyâ‚, y' - xâŸ«) + (fâ‚‚ x + âŸªyâ‚‚, y' - xâŸ« ):= add_le_add (hyâ‚ y' hy'.1) (hyâ‚‚ y' hy'.2)
      rwa [add_add_add_comm, â† inner_add_left, eq'] at this


/-! ### Optimality Theory for Unconstrained Nondifferentiable Problems -/

theorem zero_mem (hf : ConvexOn â„ s f) (h : x âˆˆ {x | âˆ€ y âˆˆ s, f x â‰¤ f y}) :
  (0 : E) âˆˆ SubderivAt hf x :=
    fun y ys => le_of_le_of_eq' (h y ys) (by rw [inner_zero_left, add_zero])

theorem isGlobalmin (hf : ConvexOn â„ s f) (h : (0 : E) âˆˆ SubderivAt hf x ) :
  x âˆˆ {x | âˆ€ y âˆˆ s, f x â‰¤ f y} := by
    intro y ys; specialize h y ys
    rwa [inner_zero_left, add_zero] at h

/-- `x'` minimize `f` if and only if `0` is a subgradient of `f` at `x'` --/
theorem zero_mem_iff_isGlobalmin (hf : ConvexOn â„ s f) :
  (0 : E) âˆˆ SubderivAt hf x â†” x âˆˆ {x | âˆ€ y âˆˆ s, f x â‰¤ f y} :=
    âŸ¨fun h => isGlobalmin hf h, fun h => zero_mem hf hâŸ©



/-! ### Convergence of Subgradient method -/
variable {G : NNReal} (hf : ConvexOn â„ s f) (lf : LipschitzWith G f)

variable (point : â„• â†’ E) (g : â„• â†’ E)
  (a : â„• â†’ â„) (ha : âˆ€ (n : â„•), a n > 0) (xâ‚€ : E)
  (hg : âˆ€ (n : â„•), g n âˆˆ SubderivAt hf (point n))

variable (update : âˆ€ (k : â„•), (point (k + 1)) = point k - a k â€¢ (g k))

variable (xm : E) (hm : IsMinOn f s xm)

/- Subgradient of `f` is bounded if and only if `f` is Lipschitz -/
theorem bounded_subgradient_iff_Lipschitz :
    âˆ€ g âˆˆ SubderivAt hf x, â€–gâ€– â‰¤ G â†” LipschitzWith G f := by sorry

theorem subgradient_method :
    âˆ€ (k : â„•), 2 * ((Finset.range (k + 1)).sum a) * (sInf {f (point i) | i âˆˆ Finset.range (k + 1)} - (f xm))
      â‰¤ â€–xâ‚€ - xmâ€– ^ 2 + G ^ 2 * (Finset.range (k + 1)).sum (fun i => (a i) ^ 2) := by sorry

theorem subgradient_method_1 {t : â„} (ha' : âˆ€ (n : â„•), a n = t) :
    âˆ€ (k : â„•), sInf {f (point i) | i âˆˆ Finset.range (k + 1)} - (f xm)
      â‰¤ â€–xâ‚€ - xmâ€– ^ 2 / (2 * k * t) + G ^ 2 * t / 2 := by sorry

theorem subgradient_method_2 {s : â„} (ha' : âˆ€ (n : â„•), a n * â€–g nâ€– = s) :
    âˆ€ (k : â„•), sInf {f (point i) | i âˆˆ Finset.range (k + 1)} - (f xm)
      â‰¤ G * â€–xâ‚€ - xmâ€– ^ 2 / (2 * k * s) + G * s / 2 := by sorry

theorem subgradient_method_3 (ha' : Tendsto a atTop (ğ“ 0))
    (ha'' : Tendsto (fun (k : â„•) => (Finset.range (k + 1)).sum a) atTop atTop) :
    Tendsto (fun k => sInf {f (point i) | i âˆˆ Finset.range (k + 1)}) atTop (ğ“ (f xm)) := by sorry

end
